<p>Vector embeddings are arguably the single most important piece of the
modern success of generative language models (GLMs) in capturing the
popular attention, and indeed making real and substantial contributions
to Artificial Intelligence research.</p>
<ol start="9" type="1">
<li>Ninth</li>
<li>Tenth</li>
</ol>
<ol start="11" type="1">
<li>Eleventh
<ol type="i">
<li>subone</li>
</ol></li>
</ol>
<ol start="2" type="i">
<li>subtwo
<ol start="3" type="i">
<li>subthree</li>
</ol></li>
</ol>
<dl>
<dt>Term 1</dt>
<dd>
<p>Definition 1</p>
</dd>
<dt>Term 2 with <em>inline markup</em></dt>
<dd>
<p>Definition 2</p>
<pre><code>{ some code, part of Definition 2 }</code></pre>
<p>Third paragraph of definition 2.</p>
</dd>
</dl>
<ol class="example" type="1">
<li>My second example will be numbered (2).</li>
<li>My first example will be numbered.</li>
</ol>
<p>Explanation of examples.</p>
<ul>
<li><p>First</p></li>
<li><p>Second:</p>
<ul>
<li>Fee</li>
<li>Fie</li>
<li>Foe</li>
</ul></li>
<li><p>Third</p></li>
<li><p>item one</p></li>
<li><p>item two</p></li>
</ul>
<!-- eoi -->
<pre><code>{ my code block }</code></pre>
<ol start="3" class="example" type="1">
<li>This is a good example.</li>
<li>My third example will be numbered (3).</li>
</ol>
<ol type="1">
<li>one
<ol type="i">
<li>two</li>
<li>too</li>
</ol></li>
<li>three</li>
</ol>
<p><!--  --></p>
<ol type="1">
<li>uno</li>
<li>dos</li>
<li>tres</li>
</ol>
<table>
<caption>Demonstration of simple table syntax.</caption>
<thead>
<tr class="header">
<th style="text-align: right;">Right</th>
<th style="text-align: left;">Left</th>
<th style="text-align: center;">Center</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">12</td>
<td style="text-align: left;">12</td>
<td style="text-align: center;">12</td>
<td>12</td>
</tr>
<tr class="even">
<td style="text-align: right;">123</td>
<td style="text-align: left;">123</td>
<td style="text-align: center;">123</td>
<td>123</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: center;">1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>See [my website].</p>
<p>As (3) illustrates, … I think one reasonable way to look at the
current state of GLMs is as increasingly sophisticated operations on a
set of vectors, where these vectors are simply the embedding used. One
of the core feats of natural-language intelligence (NLI) as exhibited by
competent natural-language speakers is performing a transformation
<em>from</em> vocables or scribables <em>to</em> meanings, and then once
suitable operations are performed in semantic-space, a transformation
back <em>from</em> meanings <em>to</em> vocables and scribables. It is
not an accident that GLMs do something very similar in terms of
transformations: taking the natural-language text or speech that humans
output, and transforming it into values in an abstract space where
further operation and computation can be performed. The major difference
is that in the machine learning (ML) case the space in question is a
high-dimensional vector-space. What happens after that transformation
into the abstract space, however, is less clearly analogous to the
operations of natural-language intelligence. As I said above, the
broadest way to characterize what happens in the NLI case is that
“suitable operations are performed in semantic-space”. You’ll hear
philosophers often call this the “space of reasons” as well. While not
something formally articulated, this notion of a semantic space gets at
the fundamental idea that meanings don’t (and can’t<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>)
operate at the level of vocables and scribables. Researchers in the GLM
space have invented ingenious methods that approximate remarkably
closely the outputs of NLI. By training models with many billions of
degrees of freedom on the input and output parts of NLI transformations,
they can approximate functions that empirically match the operation of
NLI in an extraordinary number of cases. But these functions operate on
vectors, and so it is the vectors, their generation and use, that become
of central importance.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>It’s simple to see this when you realize the many-many
relation between the uttered and written tokens of language (on one
side) and their meanings (on the other). The cases abound, and while
there are plenty of examples at the level of single words (e.g. ‘bank’,
‘arms’, ‘resolution’) the examples get more persuasive in complex
formations: ‘you can’t be too nice to her’. [my website]:
http://foo.bar.baz<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
